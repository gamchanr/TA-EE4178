{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 개론 (+Google Colab 사용법)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. Google Colab: [[링크](https://colab.research.google.com/notebooks/welcome.ipynb)] [[사용법](https://drive.google.com/file/d/11B7cjkW0KVMZv-yqxHDhg0TUE3CESYSx/view)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Basic Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 자료형과 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 숫자형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.2\n",
      "11.2 8.8 12.0 8.333333333333334\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 1.2\n",
    "print(a, b)\n",
    "print(a+b, a-b, a*b, a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is too short, Python is fast\n"
     ]
    }
   ],
   "source": [
    "quote = \"Life is too short,\"\n",
    "print(quote, \"Python is fast\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스트 / 튜플"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 'Life', ['a', 'b', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(a[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "print(a[-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "print(a[-1][1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(a[0:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 'Life', ['a', 'b', 'c'], 'short']\n"
     ]
    }
   ],
   "source": [
    "a.append('short') \n",
    "a.insert(2, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 튜플: 리스트와 비슷하지만 요소값 생성, 삭제, 수정이 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딕셔너리 / 집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'key': 'value', '류현진': '야구', '손흥민': '축구'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "야구\n"
     ]
    }
   ],
   "source": [
    "print(dic['류현진']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['key', '류현진', '손흥민'])\n"
     ]
    }
   ],
   "source": [
    "print(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['value', '야구', '축구'])\n"
     ]
    }
   ],
   "source": [
    "print(dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'value', '류현진': '야구', '손흥민': '축구', 'new key': 'new value'}\n"
     ]
    }
   ],
   "source": [
    "dic['new key'] = 'new value' \n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 제어문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if 문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 is even number\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "if a[1]%2==0:\n",
    "    print(\"{} is even number\".format(a[1]))\n",
    "elif a[1]%2==1:\n",
    "    print(\"{} is odd number\".format(a[1]))\n",
    "else:\n",
    "    print(\"Impossible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for 문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### while 문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "while True:\n",
    "    if a == 3:\n",
    "        break\n",
    "    print(a) \n",
    "    a +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 함수 / 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def square_sum(a, b):\n",
    "    squared_a = a**2\n",
    "    squared_b = b**2\n",
    "    return squared_a + squared_b\n",
    "\n",
    "c = square_sum(3, 4)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 클래스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example for initializing a Class\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "class Calculator():\n",
    "    def __init__(self):\n",
    "        self.description = \"Example for initializing a Class\"\n",
    "        self.result = 0\n",
    "\n",
    "    def add(self, num1, numb2):\n",
    "        if type(num1) == int and type(numb2) == int:\n",
    "            self.result = self.result + num1 + numb2\n",
    "            return self.result\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "cal = Calculator()\n",
    "print(cal.description)\n",
    "print(cal.add(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "print(a, type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors / Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors / Numpy / Python List\n",
    "* Python List: 행렬연산을 위해 for 문을 사용  \n",
    "* Numpy: 조건에 따라 차원이 다른 배열간의 연산을 가능하게 하는 Broadcasting을 사용하여 연산가능  \n",
    "* Tensor: pytorch의 자료형으로 numpy배열과 비슷하나, GPU에서의 연산이 가능하고, Pytorch의 Autograd(자동미분) 연산 가능 [[참고](https://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221558405051)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 4.6566e-10, 0.0000e+00],\n",
      "        [4.6566e-10, 7.3348e+28, 6.1425e+28],\n",
      "        [7.1441e+31, 6.9987e+22, 7.8675e+34],\n",
      "        [4.7418e+30, 5.9663e-02, 7.0374e+22],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화 되지 않은 행렬 생성\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8355, 0.5548, 0.2035],\n",
      "        [0.2825, 0.7910, 0.1439],\n",
      "        [0.3299, 0.6695, 0.1408],\n",
      "        [0.0692, 0.9165, 0.9834],\n",
      "        [0.0442, 0.1277, 0.4568]])\n"
     ]
    }
   ],
   "source": [
    "# 0~1 사이에서 랜덤으로 초기화된 행렬 생성\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5779, -1.2640,  0.2894],\n",
      "        [-0.6126,  0.4374, -1.1763],\n",
      "        [-0.5549, -0.6365,  2.2966],\n",
      "        [ 0.7137, -1.9657,  1.2797],\n",
      "        [ 0.3965,  0.6895,  0.2107]])\n"
     ]
    }
   ],
   "source": [
    "# Standard Normal Distribution 에서 랜덤으로 초기화된 행렬 생성\n",
    "x = torch.randn(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2000, 2.4000])\n"
     ]
    }
   ],
   "source": [
    "# 값 지정하여 행렬 생성\n",
    "x = torch.tensor([1.2, 2.4])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3289, -0.0628,  1.2464],\n",
      "        [-0.1202,  1.5995,  0.0941],\n",
      "        [-1.1099, -1.1445,  0.9576],\n",
      "        [-0.8153, -1.2154,  0.7452],\n",
      "        [-0.1476, -0.7679, -0.9896]]) \n",
      " tensor([[-0.2256,  0.4038,  0.1180],\n",
      "        [-0.0797,  1.5446,  0.4062],\n",
      "        [-0.7127,  1.6398, -0.4783],\n",
      "        [-0.9743, -1.1191,  1.6526],\n",
      "        [ 1.0673, -1.1267,  1.0245]]) \n",
      " tensor([[-0.5545,  0.3411,  1.3644],\n",
      "        [-0.1999,  3.1441,  0.5003],\n",
      "        [-1.8226,  0.4953,  0.4793],\n",
      "        [-1.7897, -2.3345,  2.3978],\n",
      "        [ 0.9198, -1.8945,  0.0349]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 3)\n",
    "print(x, '\\n', y, '\\n', x+y) # x+y = torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Size 확인\n",
    "x = torch.randn(4, 4)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# Resize - view\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# Resize - reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor, Numpy 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) <class 'torch.Tensor'>\n",
      "[1. 1. 1. 1. 1.] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Tensor to Numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, type(a))\n",
    "print(b, type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Numpy to Tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, type(a))\n",
    "print(b, type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 연산을 위한 CUDA Tensor\n",
    "a = torch.ones(5)\n",
    "# b = a.to(device=\"cudo:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Autograd  \n",
    ".requires_grad 를 true로 세팅하면, 텐서의 모든 연산에 대한 추적 가능  \n",
    "이를 통해 forward propagation 후 .backward() 호출 시 모든 gradient를 자동으로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  48.],\n",
      "        [ 75., 108.]], grad_fn=<MulBackward0>)\n",
      "tensor(64.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True) # default requires_grad for input data: False (whereas default requires_grad for each layer is True)\n",
    "y = (x+2)**2*3\n",
    "print(y)\n",
    "\n",
    "out = y.mean()\n",
    "print(out)\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because {dout\\over dx_i} = {dout \\over dy} * {dy \\over dx_i} = {1 \\over 4}*6(x_i+2) = {3\\over2}(x_i+2) \\hspace{1cm}$\n",
    "as ${dout \\over dy} = {1 \\over 4}, {dy \\over dx_i} = 6(x_i+2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch Project 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision # to download 'CIFAR10' dataset\n",
    "import torchvision.transforms as transforms # to manipulate input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='./datasets', \n",
    "                                          train=True, \n",
    "                                          transform=transforms.ToTensor(), \n",
    "                                          download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 torch.Size([3, 32, 32]) 6\n"
     ]
    }
   ],
   "source": [
    "# cf) check for the data\n",
    "image, label = train_data[0]\n",
    "print(len(train_data), image.size(), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Dataloader\n",
    "define dataloader (when iters loop, dataloader loads data from queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "torch.Size([64, 3, 32, 32]) tensor([3, 2, 2, 1, 5, 1, 5, 2, 1, 8, 7, 2, 9, 5, 5, 8, 7, 8, 1, 3, 7, 7, 7, 2,\n",
      "        8, 8, 3, 7, 2, 5, 0, 5, 0, 3, 0, 6, 2, 5, 9, 1, 2, 1, 9, 4, 9, 0, 2, 6,\n",
      "        7, 4, 2, 2, 0, 9, 5, 4, 8, 0, 9, 1, 5, 3, 2, 1])\n",
      "torch.Size([64, 3, 32, 32]) tensor([9, 7, 4, 3, 7, 0, 7, 0, 8, 0, 1, 2, 6, 9, 6, 0, 2, 7, 8, 2, 4, 9, 7, 5,\n",
      "        2, 2, 3, 5, 5, 9, 5, 6, 3, 6, 8, 8, 1, 1, 1, 1, 8, 8, 0, 2, 4, 4, 2, 7,\n",
      "        9, 9, 6, 9, 4, 1, 9, 6, 6, 5, 7, 6, 1, 8, 6, 0])\n",
      "torch.Size([64, 3, 32, 32]) tensor([3, 4, 2, 0, 2, 8, 0, 7, 6, 9, 9, 8, 9, 0, 0, 4, 2, 7, 0, 4, 2, 0, 1, 5,\n",
      "        7, 4, 4, 2, 8, 8, 5, 4, 5, 9, 8, 0, 8, 3, 7, 0, 6, 0, 8, 8, 2, 5, 3, 8,\n",
      "        8, 7, 1, 8, 9, 1, 2, 6, 5, 2, 4, 4, 2, 0, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "# cf) check how data_loader works\n",
    "print(len(data_loader))\n",
    "for idx, (images, labels) in enumerate(data_loader):\n",
    "    if idx == 3:\n",
    "        break\n",
    "    print(images.size(), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Linear(6*14*14, 10) # 10 final nodes as CIFAR10 dataset has 10 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1176, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# cf) check which layers constitute Network\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) \n",
      " tensor([-0.0841,  0.3907, -0.7210,  0.6113,  0.2720,  0.6076,  0.6780, -0.4594,\n",
      "         0.1557, -0.0570], grad_fn=<SelectBackward>) \n",
      " 6\n"
     ]
    }
   ],
   "source": [
    "# cf) check how data passes through the Network\n",
    "data_iter = iter(data_loader)\n",
    "images, labels = data_iter.next() # only get one mini-batch\n",
    "outputs = model(images)\n",
    "print(outputs.size(), '\\n', outputs[0], '\\n', outputs[0].tolist().index(max(outputs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/782], Loss: 1.8431\n",
      "Epoch [1/1], Step [2/782], Loss: 1.9104\n",
      "Epoch [1/1], Step [3/782], Loss: 1.8889\n",
      "Epoch [1/1], Step [4/782], Loss: 1.9624\n",
      "Epoch [1/1], Step [5/782], Loss: 1.9022\n",
      "Epoch [1/1], Step [6/782], Loss: 1.9324\n",
      "Epoch [1/1], Step [7/782], Loss: 1.6998\n",
      "Epoch [1/1], Step [8/782], Loss: 1.8164\n",
      "Epoch [1/1], Step [9/782], Loss: 1.8314\n",
      "Epoch [1/1], Step [10/782], Loss: 1.8615\n",
      "Epoch [1/1], Step [11/782], Loss: 1.9456\n",
      "Epoch [1/1], Step [12/782], Loss: 1.7522\n",
      "Epoch [1/1], Step [13/782], Loss: 1.9433\n",
      "Epoch [1/1], Step [14/782], Loss: 1.8207\n",
      "Epoch [1/1], Step [15/782], Loss: 1.9919\n",
      "Epoch [1/1], Step [16/782], Loss: 2.0210\n",
      "Epoch [1/1], Step [17/782], Loss: 1.8945\n",
      "Epoch [1/1], Step [18/782], Loss: 1.9919\n",
      "Epoch [1/1], Step [19/782], Loss: 1.7136\n",
      "Epoch [1/1], Step [20/782], Loss: 1.9824\n",
      "Epoch [1/1], Step [21/782], Loss: 1.9556\n",
      "Epoch [1/1], Step [22/782], Loss: 1.9255\n",
      "Epoch [1/1], Step [23/782], Loss: 1.8711\n",
      "Epoch [1/1], Step [24/782], Loss: 1.9328\n",
      "Epoch [1/1], Step [25/782], Loss: 1.9269\n",
      "Epoch [1/1], Step [26/782], Loss: 1.8404\n",
      "Epoch [1/1], Step [27/782], Loss: 1.8741\n",
      "Epoch [1/1], Step [28/782], Loss: 1.7731\n",
      "Epoch [1/1], Step [29/782], Loss: 1.8644\n",
      "Epoch [1/1], Step [30/782], Loss: 1.8209\n",
      "Epoch [1/1], Step [31/782], Loss: 1.8856\n",
      "Epoch [1/1], Step [32/782], Loss: 1.8709\n",
      "Epoch [1/1], Step [33/782], Loss: 1.9036\n",
      "Epoch [1/1], Step [34/782], Loss: 1.6174\n",
      "Epoch [1/1], Step [35/782], Loss: 1.7616\n",
      "Epoch [1/1], Step [36/782], Loss: 1.9256\n",
      "Epoch [1/1], Step [37/782], Loss: 1.9643\n",
      "Epoch [1/1], Step [38/782], Loss: 1.8410\n",
      "Epoch [1/1], Step [39/782], Loss: 1.7387\n",
      "Epoch [1/1], Step [40/782], Loss: 1.6487\n",
      "Epoch [1/1], Step [41/782], Loss: 1.8917\n",
      "Epoch [1/1], Step [42/782], Loss: 1.8084\n",
      "Epoch [1/1], Step [43/782], Loss: 1.8512\n",
      "Epoch [1/1], Step [44/782], Loss: 1.8485\n",
      "Epoch [1/1], Step [45/782], Loss: 1.9477\n",
      "Epoch [1/1], Step [46/782], Loss: 1.8063\n",
      "Epoch [1/1], Step [47/782], Loss: 1.9674\n",
      "Epoch [1/1], Step [48/782], Loss: 1.7546\n",
      "Epoch [1/1], Step [49/782], Loss: 1.7727\n",
      "Epoch [1/1], Step [50/782], Loss: 1.8261\n",
      "Epoch [1/1], Step [51/782], Loss: 1.9163\n",
      "Epoch [1/1], Step [52/782], Loss: 1.7759\n",
      "Epoch [1/1], Step [53/782], Loss: 1.9323\n",
      "Epoch [1/1], Step [54/782], Loss: 1.8526\n",
      "Epoch [1/1], Step [55/782], Loss: 1.7300\n",
      "Epoch [1/1], Step [56/782], Loss: 1.7181\n",
      "Epoch [1/1], Step [57/782], Loss: 1.9200\n",
      "Epoch [1/1], Step [58/782], Loss: 1.9889\n",
      "Epoch [1/1], Step [59/782], Loss: 1.7571\n",
      "Epoch [1/1], Step [60/782], Loss: 1.6378\n",
      "Epoch [1/1], Step [61/782], Loss: 1.7614\n",
      "Epoch [1/1], Step [62/782], Loss: 1.7463\n",
      "Epoch [1/1], Step [63/782], Loss: 1.8156\n",
      "Epoch [1/1], Step [64/782], Loss: 1.9883\n",
      "Epoch [1/1], Step [65/782], Loss: 1.7738\n",
      "Epoch [1/1], Step [66/782], Loss: 1.6735\n",
      "Epoch [1/1], Step [67/782], Loss: 1.8241\n",
      "Epoch [1/1], Step [68/782], Loss: 1.8846\n",
      "Epoch [1/1], Step [69/782], Loss: 1.8834\n",
      "Epoch [1/1], Step [70/782], Loss: 1.7862\n",
      "Epoch [1/1], Step [71/782], Loss: 1.7299\n",
      "Epoch [1/1], Step [72/782], Loss: 1.8030\n",
      "Epoch [1/1], Step [73/782], Loss: 1.9130\n",
      "Epoch [1/1], Step [74/782], Loss: 1.9762\n",
      "Epoch [1/1], Step [75/782], Loss: 1.9429\n",
      "Epoch [1/1], Step [76/782], Loss: 1.8659\n",
      "Epoch [1/1], Step [77/782], Loss: 1.9245\n",
      "Epoch [1/1], Step [78/782], Loss: 1.7275\n",
      "Epoch [1/1], Step [79/782], Loss: 1.7164\n",
      "Epoch [1/1], Step [80/782], Loss: 1.8173\n",
      "Epoch [1/1], Step [81/782], Loss: 1.9564\n",
      "Epoch [1/1], Step [82/782], Loss: 1.6987\n",
      "Epoch [1/1], Step [83/782], Loss: 1.7688\n",
      "Epoch [1/1], Step [84/782], Loss: 1.7552\n",
      "Epoch [1/1], Step [85/782], Loss: 1.9410\n",
      "Epoch [1/1], Step [86/782], Loss: 1.8465\n",
      "Epoch [1/1], Step [87/782], Loss: 1.8189\n",
      "Epoch [1/1], Step [88/782], Loss: 1.9854\n",
      "Epoch [1/1], Step [89/782], Loss: 1.9234\n",
      "Epoch [1/1], Step [90/782], Loss: 1.8988\n",
      "Epoch [1/1], Step [91/782], Loss: 1.8211\n",
      "Epoch [1/1], Step [92/782], Loss: 1.8374\n",
      "Epoch [1/1], Step [93/782], Loss: 1.8262\n",
      "Epoch [1/1], Step [94/782], Loss: 1.8961\n",
      "Epoch [1/1], Step [95/782], Loss: 1.9661\n",
      "Epoch [1/1], Step [96/782], Loss: 1.8838\n",
      "Epoch [1/1], Step [97/782], Loss: 1.8736\n",
      "Epoch [1/1], Step [98/782], Loss: 1.7253\n",
      "Epoch [1/1], Step [99/782], Loss: 2.0280\n",
      "Epoch [1/1], Step [100/782], Loss: 1.7404\n",
      "Epoch [1/1], Step [101/782], Loss: 1.8401\n",
      "Epoch [1/1], Step [102/782], Loss: 1.7735\n",
      "Epoch [1/1], Step [103/782], Loss: 1.7338\n",
      "Epoch [1/1], Step [104/782], Loss: 1.8486\n",
      "Epoch [1/1], Step [105/782], Loss: 1.6789\n",
      "Epoch [1/1], Step [106/782], Loss: 1.5960\n",
      "Epoch [1/1], Step [107/782], Loss: 1.9599\n",
      "Epoch [1/1], Step [108/782], Loss: 2.0697\n",
      "Epoch [1/1], Step [109/782], Loss: 1.7187\n",
      "Epoch [1/1], Step [110/782], Loss: 1.9518\n",
      "Epoch [1/1], Step [111/782], Loss: 1.8310\n",
      "Epoch [1/1], Step [112/782], Loss: 1.9188\n",
      "Epoch [1/1], Step [113/782], Loss: 1.7832\n",
      "Epoch [1/1], Step [114/782], Loss: 1.8526\n",
      "Epoch [1/1], Step [115/782], Loss: 1.9114\n",
      "Epoch [1/1], Step [116/782], Loss: 1.9101\n",
      "Epoch [1/1], Step [117/782], Loss: 1.8831\n",
      "Epoch [1/1], Step [118/782], Loss: 2.0838\n",
      "Epoch [1/1], Step [119/782], Loss: 1.7851\n",
      "Epoch [1/1], Step [120/782], Loss: 1.8551\n",
      "Epoch [1/1], Step [121/782], Loss: 1.8745\n",
      "Epoch [1/1], Step [122/782], Loss: 1.7658\n",
      "Epoch [1/1], Step [123/782], Loss: 1.7593\n",
      "Epoch [1/1], Step [124/782], Loss: 1.7850\n",
      "Epoch [1/1], Step [125/782], Loss: 1.8234\n",
      "Epoch [1/1], Step [126/782], Loss: 1.7956\n",
      "Epoch [1/1], Step [127/782], Loss: 1.8733\n",
      "Epoch [1/1], Step [128/782], Loss: 1.7393\n",
      "Epoch [1/1], Step [129/782], Loss: 2.0236\n",
      "Epoch [1/1], Step [130/782], Loss: 1.8259\n",
      "Epoch [1/1], Step [131/782], Loss: 1.6746\n",
      "Epoch [1/1], Step [132/782], Loss: 1.8312\n",
      "Epoch [1/1], Step [133/782], Loss: 1.9199\n",
      "Epoch [1/1], Step [134/782], Loss: 1.8057\n",
      "Epoch [1/1], Step [135/782], Loss: 1.8407\n",
      "Epoch [1/1], Step [136/782], Loss: 1.6527\n",
      "Epoch [1/1], Step [137/782], Loss: 1.8449\n",
      "Epoch [1/1], Step [138/782], Loss: 1.7978\n",
      "Epoch [1/1], Step [139/782], Loss: 1.8827\n",
      "Epoch [1/1], Step [140/782], Loss: 1.9331\n",
      "Epoch [1/1], Step [141/782], Loss: 1.9498\n",
      "Epoch [1/1], Step [142/782], Loss: 1.7084\n",
      "Epoch [1/1], Step [143/782], Loss: 1.8723\n",
      "Epoch [1/1], Step [144/782], Loss: 2.0072\n",
      "Epoch [1/1], Step [145/782], Loss: 1.7403\n",
      "Epoch [1/1], Step [146/782], Loss: 1.8561\n",
      "Epoch [1/1], Step [147/782], Loss: 1.7123\n",
      "Epoch [1/1], Step [148/782], Loss: 1.9879\n",
      "Epoch [1/1], Step [149/782], Loss: 1.8579\n",
      "Epoch [1/1], Step [150/782], Loss: 1.7219\n",
      "Epoch [1/1], Step [151/782], Loss: 1.6754\n",
      "Epoch [1/1], Step [152/782], Loss: 2.0814\n",
      "Epoch [1/1], Step [153/782], Loss: 2.0273\n",
      "Epoch [1/1], Step [154/782], Loss: 1.7751\n",
      "Epoch [1/1], Step [155/782], Loss: 1.8560\n",
      "Epoch [1/1], Step [156/782], Loss: 1.8018\n",
      "Epoch [1/1], Step [157/782], Loss: 1.9436\n",
      "Epoch [1/1], Step [158/782], Loss: 1.6905\n",
      "Epoch [1/1], Step [159/782], Loss: 1.7799\n",
      "Epoch [1/1], Step [160/782], Loss: 1.7949\n",
      "Epoch [1/1], Step [161/782], Loss: 1.9273\n",
      "Epoch [1/1], Step [162/782], Loss: 1.8589\n",
      "Epoch [1/1], Step [163/782], Loss: 1.7203\n",
      "Epoch [1/1], Step [164/782], Loss: 1.7971\n",
      "Epoch [1/1], Step [165/782], Loss: 1.7070\n",
      "Epoch [1/1], Step [166/782], Loss: 1.9588\n",
      "Epoch [1/1], Step [167/782], Loss: 1.6463\n",
      "Epoch [1/1], Step [168/782], Loss: 1.7264\n",
      "Epoch [1/1], Step [169/782], Loss: 1.8429\n",
      "Epoch [1/1], Step [170/782], Loss: 1.5844\n",
      "Epoch [1/1], Step [171/782], Loss: 1.7199\n",
      "Epoch [1/1], Step [172/782], Loss: 1.7537\n",
      "Epoch [1/1], Step [173/782], Loss: 1.7288\n",
      "Epoch [1/1], Step [174/782], Loss: 1.7460\n",
      "Epoch [1/1], Step [175/782], Loss: 1.9237\n",
      "Epoch [1/1], Step [176/782], Loss: 1.7263\n",
      "Epoch [1/1], Step [177/782], Loss: 1.7739\n",
      "Epoch [1/1], Step [178/782], Loss: 1.7304\n",
      "Epoch [1/1], Step [179/782], Loss: 1.7041\n",
      "Epoch [1/1], Step [180/782], Loss: 1.7445\n",
      "Epoch [1/1], Step [181/782], Loss: 1.8438\n",
      "Epoch [1/1], Step [182/782], Loss: 1.9822\n",
      "Epoch [1/1], Step [183/782], Loss: 1.9467\n",
      "Epoch [1/1], Step [184/782], Loss: 1.8128\n",
      "Epoch [1/1], Step [185/782], Loss: 2.0374\n",
      "Epoch [1/1], Step [186/782], Loss: 1.7138\n",
      "Epoch [1/1], Step [187/782], Loss: 1.7898\n",
      "Epoch [1/1], Step [188/782], Loss: 1.8074\n",
      "Epoch [1/1], Step [189/782], Loss: 1.5969\n",
      "Epoch [1/1], Step [190/782], Loss: 1.7741\n",
      "Epoch [1/1], Step [191/782], Loss: 1.8372\n",
      "Epoch [1/1], Step [192/782], Loss: 1.8067\n",
      "Epoch [1/1], Step [193/782], Loss: 1.6430\n",
      "Epoch [1/1], Step [194/782], Loss: 1.9055\n",
      "Epoch [1/1], Step [195/782], Loss: 1.7658\n",
      "Epoch [1/1], Step [196/782], Loss: 1.9144\n",
      "Epoch [1/1], Step [197/782], Loss: 1.9148\n",
      "Epoch [1/1], Step [198/782], Loss: 1.6278\n",
      "Epoch [1/1], Step [199/782], Loss: 1.7859\n",
      "Epoch [1/1], Step [200/782], Loss: 1.8382\n",
      "Epoch [1/1], Step [201/782], Loss: 1.8360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [202/782], Loss: 1.9329\n",
      "Epoch [1/1], Step [203/782], Loss: 1.7495\n",
      "Epoch [1/1], Step [204/782], Loss: 1.7852\n",
      "Epoch [1/1], Step [205/782], Loss: 1.7342\n",
      "Epoch [1/1], Step [206/782], Loss: 1.6246\n",
      "Epoch [1/1], Step [207/782], Loss: 1.9064\n",
      "Epoch [1/1], Step [208/782], Loss: 1.8164\n",
      "Epoch [1/1], Step [209/782], Loss: 1.8830\n",
      "Epoch [1/1], Step [210/782], Loss: 1.9131\n",
      "Epoch [1/1], Step [211/782], Loss: 1.7249\n",
      "Epoch [1/1], Step [212/782], Loss: 1.7671\n",
      "Epoch [1/1], Step [213/782], Loss: 1.7827\n",
      "Epoch [1/1], Step [214/782], Loss: 1.8729\n",
      "Epoch [1/1], Step [215/782], Loss: 1.7339\n",
      "Epoch [1/1], Step [216/782], Loss: 1.7179\n",
      "Epoch [1/1], Step [217/782], Loss: 1.9003\n",
      "Epoch [1/1], Step [218/782], Loss: 1.7990\n",
      "Epoch [1/1], Step [219/782], Loss: 1.9010\n",
      "Epoch [1/1], Step [220/782], Loss: 1.9363\n",
      "Epoch [1/1], Step [221/782], Loss: 1.7082\n",
      "Epoch [1/1], Step [222/782], Loss: 1.7400\n",
      "Epoch [1/1], Step [223/782], Loss: 1.7633\n",
      "Epoch [1/1], Step [224/782], Loss: 1.9057\n",
      "Epoch [1/1], Step [225/782], Loss: 1.8265\n",
      "Epoch [1/1], Step [226/782], Loss: 1.8338\n",
      "Epoch [1/1], Step [227/782], Loss: 1.8174\n",
      "Epoch [1/1], Step [228/782], Loss: 1.7566\n",
      "Epoch [1/1], Step [229/782], Loss: 1.8674\n",
      "Epoch [1/1], Step [230/782], Loss: 1.7327\n",
      "Epoch [1/1], Step [231/782], Loss: 1.8991\n",
      "Epoch [1/1], Step [232/782], Loss: 1.5551\n",
      "Epoch [1/1], Step [233/782], Loss: 1.9986\n",
      "Epoch [1/1], Step [234/782], Loss: 1.8196\n",
      "Epoch [1/1], Step [235/782], Loss: 1.6540\n",
      "Epoch [1/1], Step [236/782], Loss: 1.9055\n",
      "Epoch [1/1], Step [237/782], Loss: 1.7154\n",
      "Epoch [1/1], Step [238/782], Loss: 1.7558\n",
      "Epoch [1/1], Step [239/782], Loss: 1.6688\n",
      "Epoch [1/1], Step [240/782], Loss: 1.7888\n",
      "Epoch [1/1], Step [241/782], Loss: 1.9485\n",
      "Epoch [1/1], Step [242/782], Loss: 1.7479\n",
      "Epoch [1/1], Step [243/782], Loss: 1.7533\n",
      "Epoch [1/1], Step [244/782], Loss: 1.6856\n",
      "Epoch [1/1], Step [245/782], Loss: 1.8519\n",
      "Epoch [1/1], Step [246/782], Loss: 1.8105\n",
      "Epoch [1/1], Step [247/782], Loss: 1.8681\n",
      "Epoch [1/1], Step [248/782], Loss: 1.9136\n",
      "Epoch [1/1], Step [249/782], Loss: 1.6844\n",
      "Epoch [1/1], Step [250/782], Loss: 1.8661\n",
      "Epoch [1/1], Step [251/782], Loss: 1.7619\n",
      "Epoch [1/1], Step [252/782], Loss: 1.7763\n",
      "Epoch [1/1], Step [253/782], Loss: 1.9061\n",
      "Epoch [1/1], Step [254/782], Loss: 1.7231\n",
      "Epoch [1/1], Step [255/782], Loss: 1.5392\n",
      "Epoch [1/1], Step [256/782], Loss: 1.7236\n",
      "Epoch [1/1], Step [257/782], Loss: 1.7443\n",
      "Epoch [1/1], Step [258/782], Loss: 1.8588\n",
      "Epoch [1/1], Step [259/782], Loss: 1.7422\n",
      "Epoch [1/1], Step [260/782], Loss: 1.7242\n",
      "Epoch [1/1], Step [261/782], Loss: 1.8613\n",
      "Epoch [1/1], Step [262/782], Loss: 1.7864\n",
      "Epoch [1/1], Step [263/782], Loss: 1.6894\n",
      "Epoch [1/1], Step [264/782], Loss: 1.7270\n",
      "Epoch [1/1], Step [265/782], Loss: 1.7420\n",
      "Epoch [1/1], Step [266/782], Loss: 1.8605\n",
      "Epoch [1/1], Step [267/782], Loss: 1.6364\n",
      "Epoch [1/1], Step [268/782], Loss: 1.6578\n",
      "Epoch [1/1], Step [269/782], Loss: 1.8661\n",
      "Epoch [1/1], Step [270/782], Loss: 1.8359\n",
      "Epoch [1/1], Step [271/782], Loss: 1.9286\n",
      "Epoch [1/1], Step [272/782], Loss: 2.0072\n",
      "Epoch [1/1], Step [273/782], Loss: 2.0100\n",
      "Epoch [1/1], Step [274/782], Loss: 1.7977\n",
      "Epoch [1/1], Step [275/782], Loss: 1.6298\n",
      "Epoch [1/1], Step [276/782], Loss: 1.9226\n",
      "Epoch [1/1], Step [277/782], Loss: 1.7700\n",
      "Epoch [1/1], Step [278/782], Loss: 1.6219\n",
      "Epoch [1/1], Step [279/782], Loss: 1.7689\n",
      "Epoch [1/1], Step [280/782], Loss: 1.7698\n",
      "Epoch [1/1], Step [281/782], Loss: 1.6862\n",
      "Epoch [1/1], Step [282/782], Loss: 1.9253\n",
      "Epoch [1/1], Step [283/782], Loss: 1.7913\n",
      "Epoch [1/1], Step [284/782], Loss: 1.8912\n",
      "Epoch [1/1], Step [285/782], Loss: 1.9499\n",
      "Epoch [1/1], Step [286/782], Loss: 1.7795\n",
      "Epoch [1/1], Step [287/782], Loss: 1.7062\n",
      "Epoch [1/1], Step [288/782], Loss: 1.8603\n",
      "Epoch [1/1], Step [289/782], Loss: 1.8342\n",
      "Epoch [1/1], Step [290/782], Loss: 1.8137\n",
      "Epoch [1/1], Step [291/782], Loss: 1.8533\n",
      "Epoch [1/1], Step [292/782], Loss: 1.8159\n",
      "Epoch [1/1], Step [293/782], Loss: 1.6597\n",
      "Epoch [1/1], Step [294/782], Loss: 1.8355\n",
      "Epoch [1/1], Step [295/782], Loss: 1.8571\n",
      "Epoch [1/1], Step [296/782], Loss: 1.8954\n",
      "Epoch [1/1], Step [297/782], Loss: 1.8935\n",
      "Epoch [1/1], Step [298/782], Loss: 1.5881\n",
      "Epoch [1/1], Step [299/782], Loss: 1.8875\n",
      "Epoch [1/1], Step [300/782], Loss: 1.8701\n",
      "Epoch [1/1], Step [301/782], Loss: 1.7419\n",
      "Epoch [1/1], Step [302/782], Loss: 1.6334\n",
      "Epoch [1/1], Step [303/782], Loss: 1.8433\n",
      "Epoch [1/1], Step [304/782], Loss: 1.8683\n",
      "Epoch [1/1], Step [305/782], Loss: 1.7692\n",
      "Epoch [1/1], Step [306/782], Loss: 1.8243\n",
      "Epoch [1/1], Step [307/782], Loss: 1.8611\n",
      "Epoch [1/1], Step [308/782], Loss: 1.8619\n",
      "Epoch [1/1], Step [309/782], Loss: 1.8106\n",
      "Epoch [1/1], Step [310/782], Loss: 1.9010\n",
      "Epoch [1/1], Step [311/782], Loss: 1.7930\n",
      "Epoch [1/1], Step [312/782], Loss: 1.9054\n",
      "Epoch [1/1], Step [313/782], Loss: 1.7710\n",
      "Epoch [1/1], Step [314/782], Loss: 1.9431\n",
      "Epoch [1/1], Step [315/782], Loss: 1.8387\n",
      "Epoch [1/1], Step [316/782], Loss: 1.8080\n",
      "Epoch [1/1], Step [317/782], Loss: 1.9344\n",
      "Epoch [1/1], Step [318/782], Loss: 1.8331\n",
      "Epoch [1/1], Step [319/782], Loss: 1.6819\n",
      "Epoch [1/1], Step [320/782], Loss: 1.7175\n",
      "Epoch [1/1], Step [321/782], Loss: 1.9324\n",
      "Epoch [1/1], Step [322/782], Loss: 1.6023\n",
      "Epoch [1/1], Step [323/782], Loss: 1.9654\n",
      "Epoch [1/1], Step [324/782], Loss: 1.7471\n",
      "Epoch [1/1], Step [325/782], Loss: 1.8827\n",
      "Epoch [1/1], Step [326/782], Loss: 1.7106\n",
      "Epoch [1/1], Step [327/782], Loss: 1.8147\n",
      "Epoch [1/1], Step [328/782], Loss: 1.6856\n",
      "Epoch [1/1], Step [329/782], Loss: 1.6081\n",
      "Epoch [1/1], Step [330/782], Loss: 1.9492\n",
      "Epoch [1/1], Step [331/782], Loss: 1.7151\n",
      "Epoch [1/1], Step [332/782], Loss: 1.9184\n",
      "Epoch [1/1], Step [333/782], Loss: 1.7145\n",
      "Epoch [1/1], Step [334/782], Loss: 1.7025\n",
      "Epoch [1/1], Step [335/782], Loss: 1.9044\n",
      "Epoch [1/1], Step [336/782], Loss: 2.1265\n",
      "Epoch [1/1], Step [337/782], Loss: 1.8495\n",
      "Epoch [1/1], Step [338/782], Loss: 1.6489\n",
      "Epoch [1/1], Step [339/782], Loss: 1.6698\n",
      "Epoch [1/1], Step [340/782], Loss: 2.0897\n",
      "Epoch [1/1], Step [341/782], Loss: 1.7515\n",
      "Epoch [1/1], Step [342/782], Loss: 1.7782\n",
      "Epoch [1/1], Step [343/782], Loss: 1.6969\n",
      "Epoch [1/1], Step [344/782], Loss: 1.9338\n",
      "Epoch [1/1], Step [345/782], Loss: 1.7636\n",
      "Epoch [1/1], Step [346/782], Loss: 1.7725\n",
      "Epoch [1/1], Step [347/782], Loss: 1.8169\n",
      "Epoch [1/1], Step [348/782], Loss: 1.8591\n",
      "Epoch [1/1], Step [349/782], Loss: 1.9045\n",
      "Epoch [1/1], Step [350/782], Loss: 1.6323\n",
      "Epoch [1/1], Step [351/782], Loss: 1.6579\n",
      "Epoch [1/1], Step [352/782], Loss: 1.9297\n",
      "Epoch [1/1], Step [353/782], Loss: 1.7423\n",
      "Epoch [1/1], Step [354/782], Loss: 1.8524\n",
      "Epoch [1/1], Step [355/782], Loss: 1.8619\n",
      "Epoch [1/1], Step [356/782], Loss: 1.8301\n",
      "Epoch [1/1], Step [357/782], Loss: 1.9355\n",
      "Epoch [1/1], Step [358/782], Loss: 1.7905\n",
      "Epoch [1/1], Step [359/782], Loss: 1.6765\n",
      "Epoch [1/1], Step [360/782], Loss: 1.8942\n",
      "Epoch [1/1], Step [361/782], Loss: 1.7326\n",
      "Epoch [1/1], Step [362/782], Loss: 1.7229\n",
      "Epoch [1/1], Step [363/782], Loss: 1.9718\n",
      "Epoch [1/1], Step [364/782], Loss: 1.7947\n",
      "Epoch [1/1], Step [365/782], Loss: 1.8077\n",
      "Epoch [1/1], Step [366/782], Loss: 1.8417\n",
      "Epoch [1/1], Step [367/782], Loss: 1.6964\n",
      "Epoch [1/1], Step [368/782], Loss: 1.6190\n",
      "Epoch [1/1], Step [369/782], Loss: 1.9247\n",
      "Epoch [1/1], Step [370/782], Loss: 1.9376\n",
      "Epoch [1/1], Step [371/782], Loss: 1.9441\n",
      "Epoch [1/1], Step [372/782], Loss: 1.7835\n",
      "Epoch [1/1], Step [373/782], Loss: 1.7644\n",
      "Epoch [1/1], Step [374/782], Loss: 1.6953\n",
      "Epoch [1/1], Step [375/782], Loss: 1.8176\n",
      "Epoch [1/1], Step [376/782], Loss: 1.8358\n",
      "Epoch [1/1], Step [377/782], Loss: 1.7789\n",
      "Epoch [1/1], Step [378/782], Loss: 1.7355\n",
      "Epoch [1/1], Step [379/782], Loss: 1.6895\n",
      "Epoch [1/1], Step [380/782], Loss: 1.7102\n",
      "Epoch [1/1], Step [381/782], Loss: 1.6239\n",
      "Epoch [1/1], Step [382/782], Loss: 1.5944\n",
      "Epoch [1/1], Step [383/782], Loss: 1.8624\n",
      "Epoch [1/1], Step [384/782], Loss: 1.7627\n",
      "Epoch [1/1], Step [385/782], Loss: 1.7907\n",
      "Epoch [1/1], Step [386/782], Loss: 1.7503\n",
      "Epoch [1/1], Step [387/782], Loss: 1.6004\n",
      "Epoch [1/1], Step [388/782], Loss: 1.7132\n",
      "Epoch [1/1], Step [389/782], Loss: 1.8415\n",
      "Epoch [1/1], Step [390/782], Loss: 1.7268\n",
      "Epoch [1/1], Step [391/782], Loss: 1.7603\n",
      "Epoch [1/1], Step [392/782], Loss: 1.6022\n",
      "Epoch [1/1], Step [393/782], Loss: 1.7590\n",
      "Epoch [1/1], Step [394/782], Loss: 1.8141\n",
      "Epoch [1/1], Step [395/782], Loss: 1.9359\n",
      "Epoch [1/1], Step [396/782], Loss: 1.8188\n",
      "Epoch [1/1], Step [397/782], Loss: 2.0301\n",
      "Epoch [1/1], Step [398/782], Loss: 1.7055\n",
      "Epoch [1/1], Step [399/782], Loss: 1.7832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [400/782], Loss: 1.8175\n",
      "Epoch [1/1], Step [401/782], Loss: 1.7654\n",
      "Epoch [1/1], Step [402/782], Loss: 1.7492\n",
      "Epoch [1/1], Step [403/782], Loss: 1.7454\n",
      "Epoch [1/1], Step [404/782], Loss: 1.9838\n",
      "Epoch [1/1], Step [405/782], Loss: 1.8516\n",
      "Epoch [1/1], Step [406/782], Loss: 1.9333\n",
      "Epoch [1/1], Step [407/782], Loss: 1.7862\n",
      "Epoch [1/1], Step [408/782], Loss: 1.7956\n",
      "Epoch [1/1], Step [409/782], Loss: 1.7771\n",
      "Epoch [1/1], Step [410/782], Loss: 1.9314\n",
      "Epoch [1/1], Step [411/782], Loss: 1.8072\n",
      "Epoch [1/1], Step [412/782], Loss: 1.8069\n",
      "Epoch [1/1], Step [413/782], Loss: 1.6437\n",
      "Epoch [1/1], Step [414/782], Loss: 1.8694\n",
      "Epoch [1/1], Step [415/782], Loss: 1.6547\n",
      "Epoch [1/1], Step [416/782], Loss: 1.9138\n",
      "Epoch [1/1], Step [417/782], Loss: 1.5264\n",
      "Epoch [1/1], Step [418/782], Loss: 1.5880\n",
      "Epoch [1/1], Step [419/782], Loss: 1.8529\n",
      "Epoch [1/1], Step [420/782], Loss: 1.8082\n",
      "Epoch [1/1], Step [421/782], Loss: 1.6663\n",
      "Epoch [1/1], Step [422/782], Loss: 1.8213\n",
      "Epoch [1/1], Step [423/782], Loss: 1.6297\n",
      "Epoch [1/1], Step [424/782], Loss: 1.8126\n",
      "Epoch [1/1], Step [425/782], Loss: 2.0373\n",
      "Epoch [1/1], Step [426/782], Loss: 1.8088\n",
      "Epoch [1/1], Step [427/782], Loss: 1.7057\n",
      "Epoch [1/1], Step [428/782], Loss: 1.7677\n",
      "Epoch [1/1], Step [429/782], Loss: 1.6872\n",
      "Epoch [1/1], Step [430/782], Loss: 1.8878\n",
      "Epoch [1/1], Step [431/782], Loss: 1.7857\n",
      "Epoch [1/1], Step [432/782], Loss: 1.5628\n",
      "Epoch [1/1], Step [433/782], Loss: 1.8168\n",
      "Epoch [1/1], Step [434/782], Loss: 1.8010\n",
      "Epoch [1/1], Step [435/782], Loss: 1.6366\n",
      "Epoch [1/1], Step [436/782], Loss: 1.7420\n",
      "Epoch [1/1], Step [437/782], Loss: 1.7791\n",
      "Epoch [1/1], Step [438/782], Loss: 2.0398\n",
      "Epoch [1/1], Step [439/782], Loss: 1.6392\n",
      "Epoch [1/1], Step [440/782], Loss: 1.7490\n",
      "Epoch [1/1], Step [441/782], Loss: 1.6798\n",
      "Epoch [1/1], Step [442/782], Loss: 1.6817\n",
      "Epoch [1/1], Step [443/782], Loss: 1.6027\n",
      "Epoch [1/1], Step [444/782], Loss: 1.7091\n",
      "Epoch [1/1], Step [445/782], Loss: 1.7980\n",
      "Epoch [1/1], Step [446/782], Loss: 1.6992\n",
      "Epoch [1/1], Step [447/782], Loss: 1.6692\n",
      "Epoch [1/1], Step [448/782], Loss: 1.8290\n",
      "Epoch [1/1], Step [449/782], Loss: 1.7002\n",
      "Epoch [1/1], Step [450/782], Loss: 1.7908\n",
      "Epoch [1/1], Step [451/782], Loss: 1.7774\n",
      "Epoch [1/1], Step [452/782], Loss: 1.8303\n",
      "Epoch [1/1], Step [453/782], Loss: 1.6836\n",
      "Epoch [1/1], Step [454/782], Loss: 1.7626\n",
      "Epoch [1/1], Step [455/782], Loss: 1.6406\n",
      "Epoch [1/1], Step [456/782], Loss: 1.7489\n",
      "Epoch [1/1], Step [457/782], Loss: 1.8884\n",
      "Epoch [1/1], Step [458/782], Loss: 1.5551\n",
      "Epoch [1/1], Step [459/782], Loss: 2.0007\n",
      "Epoch [1/1], Step [460/782], Loss: 1.6899\n",
      "Epoch [1/1], Step [461/782], Loss: 1.7887\n",
      "Epoch [1/1], Step [462/782], Loss: 1.7668\n",
      "Epoch [1/1], Step [463/782], Loss: 1.6667\n",
      "Epoch [1/1], Step [464/782], Loss: 1.6310\n",
      "Epoch [1/1], Step [465/782], Loss: 1.8108\n",
      "Epoch [1/1], Step [466/782], Loss: 1.7165\n",
      "Epoch [1/1], Step [467/782], Loss: 1.6439\n",
      "Epoch [1/1], Step [468/782], Loss: 1.7555\n",
      "Epoch [1/1], Step [469/782], Loss: 1.6993\n",
      "Epoch [1/1], Step [470/782], Loss: 1.8440\n",
      "Epoch [1/1], Step [471/782], Loss: 1.9596\n",
      "Epoch [1/1], Step [472/782], Loss: 1.8402\n",
      "Epoch [1/1], Step [473/782], Loss: 1.7850\n",
      "Epoch [1/1], Step [474/782], Loss: 1.5823\n",
      "Epoch [1/1], Step [475/782], Loss: 1.7607\n",
      "Epoch [1/1], Step [476/782], Loss: 1.8011\n",
      "Epoch [1/1], Step [477/782], Loss: 1.6135\n",
      "Epoch [1/1], Step [478/782], Loss: 1.7198\n",
      "Epoch [1/1], Step [479/782], Loss: 1.7221\n",
      "Epoch [1/1], Step [480/782], Loss: 1.8133\n",
      "Epoch [1/1], Step [481/782], Loss: 1.8620\n",
      "Epoch [1/1], Step [482/782], Loss: 1.7445\n",
      "Epoch [1/1], Step [483/782], Loss: 1.6779\n",
      "Epoch [1/1], Step [484/782], Loss: 1.8678\n",
      "Epoch [1/1], Step [485/782], Loss: 1.7948\n",
      "Epoch [1/1], Step [486/782], Loss: 1.7001\n",
      "Epoch [1/1], Step [487/782], Loss: 1.6627\n",
      "Epoch [1/1], Step [488/782], Loss: 1.7259\n",
      "Epoch [1/1], Step [489/782], Loss: 1.9581\n",
      "Epoch [1/1], Step [490/782], Loss: 1.8621\n",
      "Epoch [1/1], Step [491/782], Loss: 1.8766\n",
      "Epoch [1/1], Step [492/782], Loss: 1.6707\n",
      "Epoch [1/1], Step [493/782], Loss: 1.7432\n",
      "Epoch [1/1], Step [494/782], Loss: 1.8525\n",
      "Epoch [1/1], Step [495/782], Loss: 1.7526\n",
      "Epoch [1/1], Step [496/782], Loss: 1.6712\n",
      "Epoch [1/1], Step [497/782], Loss: 1.7727\n",
      "Epoch [1/1], Step [498/782], Loss: 1.7156\n",
      "Epoch [1/1], Step [499/782], Loss: 1.7506\n",
      "Epoch [1/1], Step [500/782], Loss: 1.8131\n",
      "Epoch [1/1], Step [501/782], Loss: 1.6395\n",
      "Epoch [1/1], Step [502/782], Loss: 1.9090\n",
      "Epoch [1/1], Step [503/782], Loss: 1.8659\n",
      "Epoch [1/1], Step [504/782], Loss: 1.7417\n",
      "Epoch [1/1], Step [505/782], Loss: 1.6986\n",
      "Epoch [1/1], Step [506/782], Loss: 1.6738\n",
      "Epoch [1/1], Step [507/782], Loss: 1.7341\n",
      "Epoch [1/1], Step [508/782], Loss: 1.7189\n",
      "Epoch [1/1], Step [509/782], Loss: 1.6971\n",
      "Epoch [1/1], Step [510/782], Loss: 1.8999\n",
      "Epoch [1/1], Step [511/782], Loss: 1.7899\n",
      "Epoch [1/1], Step [512/782], Loss: 1.8503\n",
      "Epoch [1/1], Step [513/782], Loss: 1.7576\n",
      "Epoch [1/1], Step [514/782], Loss: 1.8305\n",
      "Epoch [1/1], Step [515/782], Loss: 1.7079\n",
      "Epoch [1/1], Step [516/782], Loss: 1.6751\n",
      "Epoch [1/1], Step [517/782], Loss: 1.9222\n",
      "Epoch [1/1], Step [518/782], Loss: 1.7853\n",
      "Epoch [1/1], Step [519/782], Loss: 1.9587\n",
      "Epoch [1/1], Step [520/782], Loss: 1.8230\n",
      "Epoch [1/1], Step [521/782], Loss: 1.7579\n",
      "Epoch [1/1], Step [522/782], Loss: 1.6476\n",
      "Epoch [1/1], Step [523/782], Loss: 1.7595\n",
      "Epoch [1/1], Step [524/782], Loss: 1.7497\n",
      "Epoch [1/1], Step [525/782], Loss: 1.7693\n",
      "Epoch [1/1], Step [526/782], Loss: 1.8480\n",
      "Epoch [1/1], Step [527/782], Loss: 1.7815\n",
      "Epoch [1/1], Step [528/782], Loss: 1.7483\n",
      "Epoch [1/1], Step [529/782], Loss: 1.6600\n",
      "Epoch [1/1], Step [530/782], Loss: 1.8994\n",
      "Epoch [1/1], Step [531/782], Loss: 1.9297\n",
      "Epoch [1/1], Step [532/782], Loss: 1.8541\n",
      "Epoch [1/1], Step [533/782], Loss: 1.7229\n",
      "Epoch [1/1], Step [534/782], Loss: 1.8224\n",
      "Epoch [1/1], Step [535/782], Loss: 1.7802\n",
      "Epoch [1/1], Step [536/782], Loss: 1.7527\n",
      "Epoch [1/1], Step [537/782], Loss: 1.5984\n",
      "Epoch [1/1], Step [538/782], Loss: 1.5659\n",
      "Epoch [1/1], Step [539/782], Loss: 1.6918\n",
      "Epoch [1/1], Step [540/782], Loss: 1.9265\n",
      "Epoch [1/1], Step [541/782], Loss: 1.7856\n",
      "Epoch [1/1], Step [542/782], Loss: 1.7828\n",
      "Epoch [1/1], Step [543/782], Loss: 1.8785\n",
      "Epoch [1/1], Step [544/782], Loss: 1.7936\n",
      "Epoch [1/1], Step [545/782], Loss: 1.9463\n",
      "Epoch [1/1], Step [546/782], Loss: 1.7120\n",
      "Epoch [1/1], Step [547/782], Loss: 1.7198\n",
      "Epoch [1/1], Step [548/782], Loss: 1.5822\n",
      "Epoch [1/1], Step [549/782], Loss: 1.7338\n",
      "Epoch [1/1], Step [550/782], Loss: 1.8703\n",
      "Epoch [1/1], Step [551/782], Loss: 1.6735\n",
      "Epoch [1/1], Step [552/782], Loss: 1.6813\n",
      "Epoch [1/1], Step [553/782], Loss: 1.8687\n",
      "Epoch [1/1], Step [554/782], Loss: 1.7424\n",
      "Epoch [1/1], Step [555/782], Loss: 1.8740\n",
      "Epoch [1/1], Step [556/782], Loss: 1.6412\n",
      "Epoch [1/1], Step [557/782], Loss: 1.6955\n",
      "Epoch [1/1], Step [558/782], Loss: 1.9331\n",
      "Epoch [1/1], Step [559/782], Loss: 1.6543\n",
      "Epoch [1/1], Step [560/782], Loss: 2.0223\n",
      "Epoch [1/1], Step [561/782], Loss: 1.7538\n",
      "Epoch [1/1], Step [562/782], Loss: 1.6329\n",
      "Epoch [1/1], Step [563/782], Loss: 1.6152\n",
      "Epoch [1/1], Step [564/782], Loss: 1.7524\n",
      "Epoch [1/1], Step [565/782], Loss: 1.5565\n",
      "Epoch [1/1], Step [566/782], Loss: 1.8574\n",
      "Epoch [1/1], Step [567/782], Loss: 1.9143\n",
      "Epoch [1/1], Step [568/782], Loss: 1.8629\n",
      "Epoch [1/1], Step [569/782], Loss: 1.7282\n",
      "Epoch [1/1], Step [570/782], Loss: 1.8337\n",
      "Epoch [1/1], Step [571/782], Loss: 1.7051\n",
      "Epoch [1/1], Step [572/782], Loss: 1.7088\n",
      "Epoch [1/1], Step [573/782], Loss: 1.8717\n",
      "Epoch [1/1], Step [574/782], Loss: 1.7144\n",
      "Epoch [1/1], Step [575/782], Loss: 1.8258\n",
      "Epoch [1/1], Step [576/782], Loss: 1.6613\n",
      "Epoch [1/1], Step [577/782], Loss: 1.7623\n",
      "Epoch [1/1], Step [578/782], Loss: 1.9826\n",
      "Epoch [1/1], Step [579/782], Loss: 1.6756\n",
      "Epoch [1/1], Step [580/782], Loss: 1.5578\n",
      "Epoch [1/1], Step [581/782], Loss: 1.7979\n",
      "Epoch [1/1], Step [582/782], Loss: 1.8808\n",
      "Epoch [1/1], Step [583/782], Loss: 1.8950\n",
      "Epoch [1/1], Step [584/782], Loss: 1.8896\n",
      "Epoch [1/1], Step [585/782], Loss: 1.7681\n",
      "Epoch [1/1], Step [586/782], Loss: 1.5983\n",
      "Epoch [1/1], Step [587/782], Loss: 1.7762\n",
      "Epoch [1/1], Step [588/782], Loss: 1.4677\n",
      "Epoch [1/1], Step [589/782], Loss: 1.7701\n",
      "Epoch [1/1], Step [590/782], Loss: 1.7526\n",
      "Epoch [1/1], Step [591/782], Loss: 1.8647\n",
      "Epoch [1/1], Step [592/782], Loss: 1.6953\n",
      "Epoch [1/1], Step [593/782], Loss: 1.6897\n",
      "Epoch [1/1], Step [594/782], Loss: 1.6014\n",
      "Epoch [1/1], Step [595/782], Loss: 1.7165\n",
      "Epoch [1/1], Step [596/782], Loss: 1.7091\n",
      "Epoch [1/1], Step [597/782], Loss: 1.7757\n",
      "Epoch [1/1], Step [598/782], Loss: 1.9311\n",
      "Epoch [1/1], Step [599/782], Loss: 1.7771\n",
      "Epoch [1/1], Step [600/782], Loss: 1.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [601/782], Loss: 1.6928\n",
      "Epoch [1/1], Step [602/782], Loss: 1.8503\n",
      "Epoch [1/1], Step [603/782], Loss: 1.7075\n",
      "Epoch [1/1], Step [604/782], Loss: 1.7075\n",
      "Epoch [1/1], Step [605/782], Loss: 1.9455\n",
      "Epoch [1/1], Step [606/782], Loss: 1.7661\n",
      "Epoch [1/1], Step [607/782], Loss: 1.5903\n",
      "Epoch [1/1], Step [608/782], Loss: 1.8007\n",
      "Epoch [1/1], Step [609/782], Loss: 1.7519\n",
      "Epoch [1/1], Step [610/782], Loss: 1.7660\n",
      "Epoch [1/1], Step [611/782], Loss: 1.5880\n",
      "Epoch [1/1], Step [612/782], Loss: 1.6895\n",
      "Epoch [1/1], Step [613/782], Loss: 1.5041\n",
      "Epoch [1/1], Step [614/782], Loss: 1.6161\n",
      "Epoch [1/1], Step [615/782], Loss: 1.6854\n",
      "Epoch [1/1], Step [616/782], Loss: 1.7065\n",
      "Epoch [1/1], Step [617/782], Loss: 1.7604\n",
      "Epoch [1/1], Step [618/782], Loss: 1.7296\n",
      "Epoch [1/1], Step [619/782], Loss: 1.5800\n",
      "Epoch [1/1], Step [620/782], Loss: 1.7018\n",
      "Epoch [1/1], Step [621/782], Loss: 1.7299\n",
      "Epoch [1/1], Step [622/782], Loss: 1.8076\n",
      "Epoch [1/1], Step [623/782], Loss: 1.7089\n",
      "Epoch [1/1], Step [624/782], Loss: 2.0042\n",
      "Epoch [1/1], Step [625/782], Loss: 1.7319\n",
      "Epoch [1/1], Step [626/782], Loss: 1.7472\n",
      "Epoch [1/1], Step [627/782], Loss: 1.7473\n",
      "Epoch [1/1], Step [628/782], Loss: 1.4063\n",
      "Epoch [1/1], Step [629/782], Loss: 1.8014\n",
      "Epoch [1/1], Step [630/782], Loss: 1.7562\n",
      "Epoch [1/1], Step [631/782], Loss: 1.5618\n",
      "Epoch [1/1], Step [632/782], Loss: 1.7574\n",
      "Epoch [1/1], Step [633/782], Loss: 1.7770\n",
      "Epoch [1/1], Step [634/782], Loss: 1.6115\n",
      "Epoch [1/1], Step [635/782], Loss: 1.7311\n",
      "Epoch [1/1], Step [636/782], Loss: 1.8805\n",
      "Epoch [1/1], Step [637/782], Loss: 1.8513\n",
      "Epoch [1/1], Step [638/782], Loss: 1.7359\n",
      "Epoch [1/1], Step [639/782], Loss: 1.7642\n",
      "Epoch [1/1], Step [640/782], Loss: 1.6853\n",
      "Epoch [1/1], Step [641/782], Loss: 1.8197\n",
      "Epoch [1/1], Step [642/782], Loss: 1.9115\n",
      "Epoch [1/1], Step [643/782], Loss: 1.6133\n",
      "Epoch [1/1], Step [644/782], Loss: 1.6945\n",
      "Epoch [1/1], Step [645/782], Loss: 1.6806\n",
      "Epoch [1/1], Step [646/782], Loss: 1.8508\n",
      "Epoch [1/1], Step [647/782], Loss: 1.6143\n",
      "Epoch [1/1], Step [648/782], Loss: 1.8917\n",
      "Epoch [1/1], Step [649/782], Loss: 1.9551\n",
      "Epoch [1/1], Step [650/782], Loss: 1.5549\n",
      "Epoch [1/1], Step [651/782], Loss: 1.5879\n",
      "Epoch [1/1], Step [652/782], Loss: 1.7515\n",
      "Epoch [1/1], Step [653/782], Loss: 1.6600\n",
      "Epoch [1/1], Step [654/782], Loss: 1.7391\n",
      "Epoch [1/1], Step [655/782], Loss: 1.7467\n",
      "Epoch [1/1], Step [656/782], Loss: 1.7167\n",
      "Epoch [1/1], Step [657/782], Loss: 1.7272\n",
      "Epoch [1/1], Step [658/782], Loss: 1.8329\n",
      "Epoch [1/1], Step [659/782], Loss: 1.6232\n",
      "Epoch [1/1], Step [660/782], Loss: 1.7856\n",
      "Epoch [1/1], Step [661/782], Loss: 1.8520\n",
      "Epoch [1/1], Step [662/782], Loss: 1.6293\n",
      "Epoch [1/1], Step [663/782], Loss: 1.6952\n",
      "Epoch [1/1], Step [664/782], Loss: 1.6387\n",
      "Epoch [1/1], Step [665/782], Loss: 1.7715\n",
      "Epoch [1/1], Step [666/782], Loss: 1.5457\n",
      "Epoch [1/1], Step [667/782], Loss: 1.9450\n",
      "Epoch [1/1], Step [668/782], Loss: 1.7877\n",
      "Epoch [1/1], Step [669/782], Loss: 1.9126\n",
      "Epoch [1/1], Step [670/782], Loss: 1.8823\n",
      "Epoch [1/1], Step [671/782], Loss: 1.6027\n",
      "Epoch [1/1], Step [672/782], Loss: 1.8533\n",
      "Epoch [1/1], Step [673/782], Loss: 1.7661\n",
      "Epoch [1/1], Step [674/782], Loss: 1.6596\n",
      "Epoch [1/1], Step [675/782], Loss: 1.7322\n",
      "Epoch [1/1], Step [676/782], Loss: 1.6537\n",
      "Epoch [1/1], Step [677/782], Loss: 1.7268\n",
      "Epoch [1/1], Step [678/782], Loss: 1.9752\n",
      "Epoch [1/1], Step [679/782], Loss: 1.6024\n",
      "Epoch [1/1], Step [680/782], Loss: 1.7189\n",
      "Epoch [1/1], Step [681/782], Loss: 1.6462\n",
      "Epoch [1/1], Step [682/782], Loss: 1.6443\n",
      "Epoch [1/1], Step [683/782], Loss: 1.6160\n",
      "Epoch [1/1], Step [684/782], Loss: 1.6083\n",
      "Epoch [1/1], Step [685/782], Loss: 1.7058\n",
      "Epoch [1/1], Step [686/782], Loss: 1.6568\n",
      "Epoch [1/1], Step [687/782], Loss: 1.5875\n",
      "Epoch [1/1], Step [688/782], Loss: 1.5904\n",
      "Epoch [1/1], Step [689/782], Loss: 1.8049\n",
      "Epoch [1/1], Step [690/782], Loss: 1.7341\n",
      "Epoch [1/1], Step [691/782], Loss: 1.8212\n",
      "Epoch [1/1], Step [692/782], Loss: 1.7517\n",
      "Epoch [1/1], Step [693/782], Loss: 1.7922\n",
      "Epoch [1/1], Step [694/782], Loss: 1.6657\n",
      "Epoch [1/1], Step [695/782], Loss: 1.6297\n",
      "Epoch [1/1], Step [696/782], Loss: 1.6188\n",
      "Epoch [1/1], Step [697/782], Loss: 1.7301\n",
      "Epoch [1/1], Step [698/782], Loss: 1.7169\n",
      "Epoch [1/1], Step [699/782], Loss: 1.7636\n",
      "Epoch [1/1], Step [700/782], Loss: 1.8373\n",
      "Epoch [1/1], Step [701/782], Loss: 1.7074\n",
      "Epoch [1/1], Step [702/782], Loss: 1.7393\n",
      "Epoch [1/1], Step [703/782], Loss: 1.7280\n",
      "Epoch [1/1], Step [704/782], Loss: 1.9025\n",
      "Epoch [1/1], Step [705/782], Loss: 1.8755\n",
      "Epoch [1/1], Step [706/782], Loss: 1.7867\n",
      "Epoch [1/1], Step [707/782], Loss: 1.8064\n",
      "Epoch [1/1], Step [708/782], Loss: 1.6278\n",
      "Epoch [1/1], Step [709/782], Loss: 1.9065\n",
      "Epoch [1/1], Step [710/782], Loss: 1.7636\n",
      "Epoch [1/1], Step [711/782], Loss: 1.6901\n",
      "Epoch [1/1], Step [712/782], Loss: 1.8602\n",
      "Epoch [1/1], Step [713/782], Loss: 1.7788\n",
      "Epoch [1/1], Step [714/782], Loss: 1.7438\n",
      "Epoch [1/1], Step [715/782], Loss: 1.6537\n",
      "Epoch [1/1], Step [716/782], Loss: 1.7538\n",
      "Epoch [1/1], Step [717/782], Loss: 1.7521\n",
      "Epoch [1/1], Step [718/782], Loss: 1.8211\n",
      "Epoch [1/1], Step [719/782], Loss: 1.6802\n",
      "Epoch [1/1], Step [720/782], Loss: 1.8674\n",
      "Epoch [1/1], Step [721/782], Loss: 1.6876\n",
      "Epoch [1/1], Step [722/782], Loss: 1.6057\n",
      "Epoch [1/1], Step [723/782], Loss: 1.7425\n",
      "Epoch [1/1], Step [724/782], Loss: 1.8676\n",
      "Epoch [1/1], Step [725/782], Loss: 1.7337\n",
      "Epoch [1/1], Step [726/782], Loss: 1.8115\n",
      "Epoch [1/1], Step [727/782], Loss: 1.5811\n",
      "Epoch [1/1], Step [728/782], Loss: 1.6431\n",
      "Epoch [1/1], Step [729/782], Loss: 2.1183\n",
      "Epoch [1/1], Step [730/782], Loss: 1.7154\n",
      "Epoch [1/1], Step [731/782], Loss: 1.7716\n",
      "Epoch [1/1], Step [732/782], Loss: 1.5642\n",
      "Epoch [1/1], Step [733/782], Loss: 1.7189\n",
      "Epoch [1/1], Step [734/782], Loss: 1.4337\n",
      "Epoch [1/1], Step [735/782], Loss: 1.6717\n",
      "Epoch [1/1], Step [736/782], Loss: 1.7558\n",
      "Epoch [1/1], Step [737/782], Loss: 1.6093\n",
      "Epoch [1/1], Step [738/782], Loss: 1.7733\n",
      "Epoch [1/1], Step [739/782], Loss: 1.7766\n",
      "Epoch [1/1], Step [740/782], Loss: 1.7060\n",
      "Epoch [1/1], Step [741/782], Loss: 1.6994\n",
      "Epoch [1/1], Step [742/782], Loss: 1.7401\n",
      "Epoch [1/1], Step [743/782], Loss: 1.6563\n",
      "Epoch [1/1], Step [744/782], Loss: 1.9997\n",
      "Epoch [1/1], Step [745/782], Loss: 1.8020\n",
      "Epoch [1/1], Step [746/782], Loss: 1.7602\n",
      "Epoch [1/1], Step [747/782], Loss: 1.6080\n",
      "Epoch [1/1], Step [748/782], Loss: 1.6995\n",
      "Epoch [1/1], Step [749/782], Loss: 1.7010\n",
      "Epoch [1/1], Step [750/782], Loss: 1.5433\n",
      "Epoch [1/1], Step [751/782], Loss: 1.9397\n",
      "Epoch [1/1], Step [752/782], Loss: 1.9274\n",
      "Epoch [1/1], Step [753/782], Loss: 1.7858\n",
      "Epoch [1/1], Step [754/782], Loss: 1.9535\n",
      "Epoch [1/1], Step [755/782], Loss: 1.8371\n",
      "Epoch [1/1], Step [756/782], Loss: 1.7200\n",
      "Epoch [1/1], Step [757/782], Loss: 1.7089\n",
      "Epoch [1/1], Step [758/782], Loss: 1.7766\n",
      "Epoch [1/1], Step [759/782], Loss: 1.7398\n",
      "Epoch [1/1], Step [760/782], Loss: 1.5114\n",
      "Epoch [1/1], Step [761/782], Loss: 1.7345\n",
      "Epoch [1/1], Step [762/782], Loss: 1.6926\n",
      "Epoch [1/1], Step [763/782], Loss: 1.8169\n",
      "Epoch [1/1], Step [764/782], Loss: 1.8680\n",
      "Epoch [1/1], Step [765/782], Loss: 1.6108\n",
      "Epoch [1/1], Step [766/782], Loss: 1.7486\n",
      "Epoch [1/1], Step [767/782], Loss: 1.7526\n",
      "Epoch [1/1], Step [768/782], Loss: 1.6517\n",
      "Epoch [1/1], Step [769/782], Loss: 1.6827\n",
      "Epoch [1/1], Step [770/782], Loss: 1.6602\n",
      "Epoch [1/1], Step [771/782], Loss: 1.7485\n",
      "Epoch [1/1], Step [772/782], Loss: 1.6406\n",
      "Epoch [1/1], Step [773/782], Loss: 1.8385\n",
      "Epoch [1/1], Step [774/782], Loss: 1.6955\n",
      "Epoch [1/1], Step [775/782], Loss: 1.6596\n",
      "Epoch [1/1], Step [776/782], Loss: 1.6642\n",
      "Epoch [1/1], Step [777/782], Loss: 1.5473\n",
      "Epoch [1/1], Step [778/782], Loss: 1.7013\n",
      "Epoch [1/1], Step [779/782], Loss: 1.6476\n",
      "Epoch [1/1], Step [780/782], Loss: 1.7149\n",
      "Epoch [1/1], Step [781/782], Loss: 1.7534\n",
      "Epoch [1/1], Step [782/782], Loss: 1.6672\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Forward Propagate\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get Loss, Compute Gradient, Update Parameters\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, len(data_loader), loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mdatasets\u001b[m\u001b[m            intro_pytorch.ipynb model.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "# cf) check the saved model\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
